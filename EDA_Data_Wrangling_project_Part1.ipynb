{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stage-0: Import required modules such as pandas,numpy and so on\n",
    "Stage-1: Reading File(CSV, EXCEL, Tab Seperated, Parquet, pickle, feather, JSON, HTML, XML)\n",
    "         creating dataframe from data.\n",
    "Stage-2: Data Understanding: Shape, Index, Columns, info, describe, head, tail, nunique, size, sample,dtypes, duplicated\n",
    "            \n",
    "\n",
    "## Imported Required libraries, ggplot style encompasses clarity in data representation, easily understandable, visually appealing\n",
    "## with set_option we can set max rows or columns to be displayed or can also set the float format.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Read the csv file\n",
    "\n",
    "df=pd.read_csv(\"Airbnb_NYC_2019.csv\\AB_NYC_2019.csv\")\n",
    "\n",
    "\n",
    "# Found the no of rows and columns\n",
    "\n",
    "df.shape\n",
    "\n",
    "## Found some NaN values, id and host_id are different\n",
    "\n",
    "df.head(6)\n",
    "\n",
    "df.tail(5)\n",
    "\n",
    "## id and host_id are primary keys that contains unique values.\n",
    "\n",
    "df.info()\n",
    "\n",
    "## mean of price is 152, max is 10000, min is 0\n",
    "## mean of no of reviews is 23, max is 629, min is 0\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df.size\n",
    "\n",
    "## only 3 unique values for room_type\n",
    "## id is the only col with unique values\n",
    "## neighbor group have only 5 distinct values\n",
    "\n",
    "df.nunique()\n",
    "\n",
    "## index values\n",
    "\n",
    "df.index\n",
    "\n",
    "## Column values\n",
    "\n",
    "df.columns\n",
    "\n",
    "## data types\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "## same random state allow us to get same sample values everytime we run\n",
    "\n",
    "df.sample(3, random_state=42)\n",
    "\n",
    "Stage-3: Handling Null values: Remove Duplicates, Fill,Drop,Interpolate Nan or None values, isna()  \n",
    "Stage-4: Save dataframe into CSV,Excel, Copy Data, Creating new df from existing df by pointing to it.  \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "## Sum of Null values per each coln\n",
    "## last rvw, rvw month have higher no of missing values.\n",
    "## name and hst name have very less missing values such that those rows can be dropped.\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "## Filling the NaN values with 0.00 for float data\n",
    "\n",
    "df['reviews_per_month']=df['reviews_per_month'].fillna(0.00)\n",
    "\n",
    "# checking to see if there are anymore NaN values\n",
    "\n",
    "df['reviews_per_month'].isna().sum()\n",
    "\n",
    "## Dropping the column as it doesnt seem important\n",
    "\n",
    "df=df.drop('last_review',axis=1)\n",
    "\n",
    "## Checking for remaining null values\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "## Dropping remaining null values as they are less in number\n",
    "\n",
    "df=df.dropna()\n",
    "\n",
    "## checking for null values after dropping few rows and cols and filling with 0.0\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "## Checking for duplicates, Duplicated returns only second row with all the same values of first row\n",
    "## Duplicates are 0 on this df as id column holds all unique values\n",
    "## Duplicates can be applied on subset of columns as well\n",
    "\n",
    "df.duplicated().sum()\n",
    "\n",
    "## duplicated can be used to find duplicates in specific column\n",
    "\n",
    "df['host_name'].duplicated().sum()\n",
    "\n",
    "df['host_name'].duplicated().sample(5)\n",
    "\n",
    "## duplicated columns or rows can be dropped or removed using drop_duplicates()\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "## saving cleaned df as csv file in the present folder\n",
    "\n",
    "df.to_csv('Airbnb_cleaned_df.csv')\n",
    "\n",
    "## Copying one df into another and creating new df from existing df\n",
    "\n",
    "df1=df.sample(5000).copy().reset_index()\n",
    "\n",
    "df1.shape\n",
    "\n",
    "Stage-5: Data Selection: iloc, loc, at, iat, .,  \n",
    "\n",
    "        \n",
    "\n",
    "df1.head(5)\n",
    "\n",
    "# To access 1 row and 1 column using integer location\n",
    "\n",
    "df1.iloc[1,2]\n",
    "\n",
    "# To access 1 row and series of columns\n",
    "\n",
    "df1.iloc[1,2:4]\n",
    "\n",
    "# To access series of row and 1 column\n",
    "\n",
    "df1.iloc[1:2,4]\n",
    "\n",
    "# To access series of row and columns\n",
    "\n",
    "df1.iloc[1:2, 5:6]\n",
    "\n",
    "## To access multiple rows and columns\n",
    "\n",
    "df1.iloc[[1,4],[2,4]]\n",
    "\n",
    "## To access row and column using index name\n",
    "\n",
    "df1.loc[1, 'name']\n",
    "\n",
    "## To access series of rows and column using index name\n",
    "\n",
    "df1.loc[1:4, 'name']\n",
    "\n",
    "## To access row and series of column using index name\n",
    "\n",
    "df1.loc[4, 'name':'host_name']\n",
    "\n",
    "## To access series of row and column using index name\n",
    "\n",
    "df1.loc[1:4, 'name':'host_name']\n",
    "\n",
    "## To access multiple rows and columns using index name in df1\n",
    "\n",
    "df1.loc[[1,4],['name','host_name']]\n",
    "\n",
    "## at can only access 1 row and 1 column unlike loc, which can access more than one row and column \n",
    "## at is used for faster access of one row and one column\n",
    "\n",
    "df1.at[1,'name']\n",
    "\n",
    "## iat can only access 1 row and 1 column unlike iloc, which can access more than one row and column \n",
    "## iat is used for faster access of one row and one column\n",
    "\n",
    "df1.iat[1,4]\n",
    "\n",
    "Stage-6: Data Filtering: isin, Query, between, Boolean indexing, Complex Filtering with Regex and contains, logical func(and, or)  \n",
    "Stage-7: Data Transformation: lambda fun, defining function with logic  \n",
    "Stage-8: Data Removal and Data Rearrangement: rename, drop, remove, rearrange columns, duplicated  \n",
    " \n",
    "\n",
    "## isin is used to filter the data in pandas, This is used to check specific values in whole df.\n",
    "## Values that we are checking should be in list\n",
    "## It returns the entire df, with the boolean values.\n",
    "\n",
    "df1.isin(['Tara & Carl'])\n",
    "\n",
    "## To find values that doesnot contain mentioned value, use not symbol.\n",
    "\n",
    "~df1.isin(['Tara & Carl'])\n",
    "\n",
    "## To check values in the specific columns, include columns in dictionary and values in list.\n",
    "\n",
    "df1.isin({'host_name':['Tara & Carl'],'host_id':[38263259]})\n",
    "\n",
    "\n",
    "\n",
    "## Applying logical functions while filtering data, retrieve data that satisfies either one of the conditions.\n",
    "\n",
    "df1[(df1['host_name'].isin(['Tara & Carl'])) | (df1['host_id'].isin([38263259]))]\n",
    "\n",
    "## iloc and loc can alos be used to filter data, In this case, iloc retrieves data that satisfied the condition.\n",
    "\n",
    "df1[(df1.iloc[:,5]=='Brooklyn')&(df1.iloc[:,4]=='Sorel')]\n",
    "\n",
    "## Query is powerful and concise way to filter rows in pandas, It is similar to where in SQL.\n",
    "## df.query('condition'), Condition can be one or multiple by including logical functions.\n",
    "\n",
    "df1.query('host_name == \"Amy\"')\n",
    "\n",
    "## Can use any relational operators such as <,>,==,!=\n",
    "\n",
    "df1.query('price>85')\n",
    "\n",
    "## Combine multiple conditions with logical operators such as and ,or\n",
    "\n",
    "df1.query('host_name == \"Amy\" & price>85' )\n",
    "\n",
    "## between works same as in SQL, whereas and is replaced by ',' in syntax\n",
    "\n",
    "df1[df1['price'].between (10, 100)]\n",
    "\n",
    "## Boolean indexing is a powerful technique for complex filtering\n",
    "## This will have one or multiple conditions using string or logical operators or relational operators inside df[]\n",
    "\n",
    "df[(df['price']>100) & (df['host_name'].isin(['Amy']))]\n",
    "\n",
    "\n",
    "## Boolean indexing with loc, This gives result same as the above cell with df[]\n",
    "\n",
    "df.loc[(df['price']>100) & (df['host_name'].isin(['Amy']))]\n",
    "\n",
    "## Boolena indexing With iloc should contain values\n",
    "\n",
    "df.iloc[((df['price']>100) & (df['host_name'].isin(['Amy']))).values]\n",
    "\n",
    "## Lambda functions in pandas provides a concise way to perform quick calculations or transformations without defining a seperate function\n",
    "## With lambda functions, using apply, a logic can be applied to every element in df or every element in specific column\n",
    "\n",
    "df2=pd.DataFrame()\n",
    "\n",
    "df2['price']=df1['price'].apply(lambda x:x+1)\n",
    "\n",
    "df2.sample(10)\n",
    "\n",
    "## using lambda with assign we can create a new column with logic, x refers to dataframe.\n",
    "\n",
    "df2=df2.assign(price1= lambda x:x['price']+x['price']*2)\n",
    "\n",
    "## applymap is used for element wise opeartions, i.e. every element in dataframe, x refers to dataframe.\n",
    "\n",
    "df2.applymap(lambda x: x**2)\n",
    "\n",
    "## map is used to work on a series, Here x refers to series i.e. df2['price1']\n",
    "\n",
    "df2['price_categ']=df2['price1'].map(lambda x: 'High' if x>100 else 'Low')\n",
    "\n",
    "## Defining function and calling it, defined function is applied using 'apply'\n",
    "\n",
    "def sq_func(x):\n",
    "    return x**2\n",
    "\n",
    "df2['price2']= df2['price'].apply(sq_func)\n",
    "\n",
    "## inplace is optional, if you want changes to be reflected in df then keep it true otherwise false\n",
    "\n",
    "df2.rename(columns={'price2': 'price_2'}, inplace=True)\n",
    "\n",
    "## using the axis parameter to rename all columns to lower case.\n",
    "\n",
    "df.rename(str.lower, axis='columns')\n",
    "\n",
    "## without axis, all column names can be changed to upper case this way.\n",
    "\n",
    "df2=df2.rename(columns=str.upper)\n",
    "\n",
    "df2\n",
    "\n",
    "## duplicates can be dropped directly\n",
    "\n",
    "df2.drop_duplicates()\n",
    "\n",
    "## drop can be used to drop rows or columns and can also apply inplace to reflect changes in df\n",
    "## drop can also be used to drop rows or columns only when condition is satisfied.\n",
    "## inplace can be used or axis can be used in place of columns\n",
    "## to drop rows, index is optional, index need not to be mentioned, if index or column is not mentioned, by default rows will be dropped\n",
    "\n",
    "df2.drop(columns=['PRICE_2'])\n",
    "\n",
    "## Rearranging columns by converting them into list.\n",
    "\n",
    "cols=list(df1.columns.values)\n",
    "df1=df1[cols[0:5] + [cols[10]] + cols[5:10]+cols[11:16]]\n",
    "df1.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
